---
title: 'Homework 2'
header-includes:
  - \usepackage{multirow}
output:
  pdf_document: default
urlcolor: blue
---

```{r, include=FALSE}

library(tidyverse)
knitr::opts_chunk$set(tidy = FALSE)
```

## Context

This assignment reinforces ideas in Module 2: Optimization. We focus specifically on implementing the Newton's method, EM, and MM algorithms.


## Due date and submission

Please submit (via Canvas) a PDF containing a link to the web address of the GitHub repo containing your work for this assignment; git commits after the due date will cause the assignment to be considered late. Due date is Wednesday, 2/19 at 10:00AM.



## Points

```{r, echo = FALSE}
tibble(
  Problem = c("Problem 0", "Problem 1", "Problem 2 ", "Problem 3", "Problem 4"),
  Points = c(15, 30, 5, 30, 20)
) %>%
  knitr::kable()
```


## Problem 0 

This “problem” focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files.

To that end:

* create a public GitHub repo + local R Project; I suggest naming this repo / directory bios731_hw2_YourLastName (e.g. bios731_hw2_wrobel for Julia)
* Submit your whole project folder to GitHub 
* Submit a PDF knitted from Rmd to Canvas. Your solutions to the problems here should be implemented in your .Rmd file, and your git commit history should reflect the process you used to solve these Problems.


## Algorithms for logistic regression 

For a given subject in a study, we are interested in modeling $\pi_i = P(Y_i = 1|X_i = x_i)$, where $Y_i \in \{0, 1\}$. The logistic regression model takes the form

<br>

$$\text{logit}(\pi_i) = \log \left(\frac{\pi_i}{1-\pi_i}\right) = \log\left({\frac{P(Y_i = 1|X_i)}{1-P(Y_i = 1|X_i)}}\right) = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \ldots + \beta_pX_{pi}$$

* $Y_1, Y_2,\ldots, Y_n \sim Bernoulli(\pi)$ 
* PDF is $f(y_i; \pi) = \pi^{y_i}(1-\pi)^{1-y_i}$

### Problem 1: Newton's method


- Derive likelihood, gradient, and Hessian for logistic regression for an arbitrary number of predictors $p$
- What is the Newton's method update for $\beta$ for logistic regression?


- Is logistic regression a convex optimization problem? Why or why not?


### Problem 2: MM

(A) In constructing a minorizing function, first prove the inequality

$$-\log\{1 + \exp{x_i^T\theta}\} \ge -\log\{1 + \exp(X_i^T\theta^{(k)})\} - \frac{\exp(X_i^T\theta) - \exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}$$
with equality when $\theta = \theta^{(k)}$. This eliminates the log terms.


<br>

(B) Now apply the arithmetic-geometric mean inequality to the exponential function $\exp(X_i^T\theta)$ to separate the parameters. Assuming that $\theta$ has $p$ components and that there are $n$ observations, show that these maneuvers lead to a minorizing function

$$g(\theta|\theta^{(k)}) = -\frac{1}{p}\sum_{i = 1}^n \frac{\exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}\sum_{j = 1}^p\exp\{pX_{ij}(\theta_j-\theta_j^{(k)})\} + \sum_{i = 1}^nY_iX_i^T\theta = 0$$

up to a constant that does not depend on $\theta$.


(C) Finally, prove that maximizing $g(\theta|\theta^{(k)})$ consists of solving the equation

$$ -\sum_{i = 1}^n \frac{\exp(X_i^T\theta^{(k)})X_{ij}\exp(-pX_{ij}\theta_j^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}\exp(pX_{ij}\theta_j) + \sum_{i = 1}^nY_iX_{ij} = 0$$

for each $j$.

### Problem 3: simulation

Next we will implement logistic regression in R four different ways and compare the results using a short simulation study.


- implement using Newton's method from 1.1 in R
- implement using MM from 1.2 in R
- implement using `glm()` in R
- implement using `optim()` in R:
  - Use the option `method = "BFGS"`, which implements a Quasi-Newton approach



Simulation study specification:

- simulate from the model $logit(P(Y_i = 1|X_i)) = \beta_0 + \beta_1X_i$
  - $\beta_0 = 1$
  - $\beta_1 = 0.3$
  - $X_i \sim N(0, 1)$
  - $n = 200$
  - $nsim = 1$
- For your implementation of MM and Newton's method, select your own starting value and stopping criterion, but make sure they are the same for the two algorithms


You only need to run the simulation using **one simulated dataset**.  For each of the four methods, report:  

  - $\hat{\beta}_0$, $\hat{\beta}_1$
  - 95% confidence intervals for $\hat{\beta}_0$, $\hat{\beta}_1$
  - computation time
  - number of iterations to convergence


Make 2-3 plots or tables comparing your results, and summarize these findings in one paragraph.



## Problem 4: EM algorithm for censored exponential data

This will be a continuation of the lab problem on EM for censored exponential data. Suppose we have survival times $t_1, \ldots, t_n \sim Exponential(\lambda)$. 

* Do not observe all survival times because some are censored at times $c_1, \ldots, c_n$.  
* Actually observe $y_1, \ldots, y_n$, where $y_i = min(y_0, c_i)$
  * Also have an indicator $\delta_i$ where $\delta_i = 1$ is $y_i \le c_i$
    * i.e. $\delta_i = 1$ if not censored and $\delta_i = 0$ if censored
    
Do the following:

* Derive an EM algorithm to estimate the parameter $\lambda$.  Show your derivation here and report updates for the **E-step** and **M-Step**.
* Implement your EM in R and fit it to the `veteran` dataset from the `survival` package.  
  * Report your fitted $\lambda$ value. How did you monitor convergence?
  * Report a 95% confidence interval for $\lambda$, and explain how it was obtained.
  * Compare 95% confidence interval and results from those obtained by fitting an accelerated failure time model (AFT) in R with exponential errors.  You can fit an AFT model using the `phreg()` function from the `survival` package.  If you choose `dist = "weibull` and `shape = 1` as parameter arguments, this will provide exponential errors.
  



## Extra credit (up to 10 points)! Expected vs. observed information


**Part A**: Show that the expected and observed information are equivalent for logistic regression

<br>
**Part B**: Let's say you are instead performing probit regression, which is similar to logistic regression but with a different link function.  Specifically, probit regression uses a probit link: 


$$\Phi^{-1}(Pr[Y_i = 1 |X_i]) = X_i^T\beta,$$

where $\Phi^{-1}$ is inverse of the CDF for the standard normal distribution.  **Are the expected and observed information equivalent for probit regression?** Justify why or why not.
